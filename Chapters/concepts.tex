\chapter{New Concepts}
\label{chp_new_concepts}
The XGEE visualization verification algorithms are essential for interpreting and optimizing diagram structures within the editor.
While the original implementations demonstrated functionality in basic test cases, their limitations prevented comprehensive detection across various diagram types and complexities. This chapter introduces enhanced methods to address these limitations and significantly increase detection accuracy, versatility and stability.
\section{Edge Detection}
\label{chp_edge_detection}
Edges represent connections between vertices, inputs and outputs. Detecting edges accurately requires an approach that can identify individual line segments and how they connect to form complex chains. This section introduces a robust edge detection pipeline leveraging multiple computer vision techniques to reliably identify edges across a wide variety of block diagrams within XGEE.

\subsection{Methology}
\begin{wrapfigure}{R}{0.4\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Pictures/kernel_ver.png}
    \caption{kernel for vertical edges}
    \label{fig_kernel_ver}
\end{wrapfigure}
Since there are only vertical and horizontal edges within XGEE, two kernels will be used to find all pixels containing either part of a vertical or horizontal edge. The \textit{filter2D()} function places the kernel anchor (usually the top left value of the kernel) on top of a pixel, with the rest of the kernel overlapping the corresponding local pixels.\\
The kernel values are then multiplied by the corresponding pixel values underneath and added together. The result is saved and placed on the location of the anchor. The kernel on the right \ref{fig_kernel_ver} is rotated by 90\textdegree  to generate the horizontal kernel. The same process can be written as equation \ref{eq_filter2D}, where $H$ is the resulting matrix, $I$ is the original image and $K$ is the used kernel. This process is repeated for every pixel and, depending on the structure of the kernel, it can also be used to blur or sharpen an image \cite{web_filter2D}.\\
Each value in the processed image is normalized to an 8-bit integer between 0 and 255. This allows the data to be visualized as a gray scale image \ref{fig_filter2d} and processed using OpenCV's \textit{threshholding()} function \ref{fig_threshhold}. As illustrated in figure \ref{fig_comparison_filter}, ports and letters are sometimes misidentified as edges. Additionally, intersections of edges and points where horizontal and vertical edges meet are not immediately detected. These challenging areas will be processed individually in a later step in the edge detection pipeline. Apart from these specific cases, the method effectively extracts all pixels corresponding to vertical and horizontal edges, provided their widths match those of the used kernels.
\begin{equation}
\label{eq_filter2D}
    H(x, y) = \sum_{i=0}^{M_i-1} \sum_{j=0}^{M_j-1} I(x + i - a_i,y + j - a_j)K(i, j)
\end{equation}

\newpage

\begin{figure}[htb]
    \centering
    \includegraphics[width=1\linewidth]{Pictures/filter2D.png}
    \caption{Filter2D function results}
    \label{fig_filter2d}

    \centering
    \includegraphics[width=1\linewidth]{Pictures/threshhold.png}
    \caption{Threshholded filter2D function results}
    \label{fig_threshhold}
\end{figure}
For easier processing and data storage, the threshholded pixels \ref{fig_threshhold} are converted to line segments consisting of start- and endpoints. This is achieved through two of OpenCV's built-in functions:\\
\textit{findContours()}, which retrieves contours from a binary image using an algorithm introduced in this paper: \cite{art_findContours_algorithm}.\\
\textit{approxPolyDP()}, which approximates a curve or a polygon with another curve or polygon with less vertices using an algorithm introduced in this paper: \cite{art_approx_Poly_DP_algorithm}.
\begin{figure}[h]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Pictures/thresh_zoom.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Pictures/line_segments_zoom.png}
  \end{minipage}
  \caption{Comparison of detected pixels and line segments around an output before and after filtering short segments.}
  \label{fig_comparison_filter}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Pictures/intersection_peak.png}
    \caption{Peak is visible at intersection}
    \label{fig_intersection_peak}
\end{figure}\\
Misidentified letters and ports are removed by filtering out all line segments with a length shorter than 20 pixels. In all test cases, this approach successfully removes the unwanted line segments while keeping the edges \ref{fig_comparison_filter}.\\
As shown in \ref{fig_threshhold}, the \textit{filter2D()} function initially does not detect any edges at intersections, leading to gaps between the line segments. To process these gaps, it is assumed that intersections always consist of two straight edges. Overlapping 90° turns are considered impossible and will result in an error, forcing the user to maintain a clear diagram structure.\\
First, all intersections in the image are detected using OpenCV's \textit{matchTemplate()} function, which matches a template to overlapping regions of the image (Figure \ref{tm_sqdiff_normed}).\\
The function slides the template across the image, comparing overlapping patches with the template using a specified method. Among the available methods \cite{comparison_methods}, tm\_sqdiff\_normed produced the most accurate results. For each pixel, the function calculates and assigns a value representing the similarity between the template and the corresponding image region. While this approach is more precise than \textit{filter2D()}, it is also significantly more resource-intensive.
\begin{equation}
\label{tm_sqdiff_normed}
    R(x,y) = \frac{\sum_{x', y'}{(T(x', y') - I(x + x', y + y'))^2}}{\sqrt{\sum_{x', y'}{(T(x', y')^2 * \sum_{x', y'}{I(x + x', y + y')^2}}}}
\end{equation}
\begin{wrapfigure}{R}{0.2\textwidth}
\centering
\includegraphics[width=0.2\textwidth]{Pictures/kernel_intersection.png}
\caption{\label{fig_cross}}
% Template used to detect intersections
\end{wrapfigure}
At intersections, the similarity value is approximately 85\%. This slight discrepancy likely arises from how modern operating systems use aliasing to render text and lines with higher apparent resolution and contrast compared to the display. Zooming in reveals that the white pixels near edges are often replaced with subtle color hues or shades of gray. Rendering the results of the template matching highlights a peak in similarity at the intersection (Figure \ref{fig_intersection_peak}). Thresholding isolates this peak, typically yielding two or more matches per intersection. These matches are then filtered based on proximity, ensuring only one match is detected at each intersection.\\
The method processes each detected intersection by connecting the two vertical and two horizontal line segments. This eliminates any residual points near the intersection, leaving only one vertical and one horizontal line segment \ref{fig:_intersection_before_after}.\\
In the allocations editor, the same approach is applied to detect and process signal containers on edges. To enhance diagram readability, it is assumed that no 90° turns are concealed behind the containers and that edges pass through them in a straight line. The primary distinction from intersection detection lies in the number of line segments: at a signal container, only two line segments meet, rather than four.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Pictures/intersection_before_after.png}
    \caption{Intersection before and after connecting the line segments.}
    \label{fig:_intersection_before_after}
\end{figure}\\
Converting the line segments into polylines at this stage would produce unusable data, as the segments are not grouped and not sorted in the sequential order of the edge's \textit{flow}. To generate proper polylines, the line segments must first be sorted into the correct order. For example, if the first line segment in the list is an intermediate line segment within an edge, the polyline function would incorrectly attempt to connect the intermediate points directly to the endpoints, leading to incorrect detections.\\
The implemented method begins by selecting the first line segment, adding it as a starting point to the first chain and to a list of used segments and setting the chain\_growing flag to true. It then iterates through the remaining segments, checking whether each segment has already been used and whether any of its points lie within 7 pixels of the endpoints of the current segment. If a match is found, the segment is added to the used\_segments list and the current chain. If no segment is found within the 7-pixel threshold, the chain\_growing flag is set to false, and the completed chain is added to the list of chains. This process continues until all line segments have been assigned to a chain \ref{fig_chains_before_after}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Pictures/chains_before_after.png}
    \caption{Left: detected vertical and horizontal line segments\\Right: detected line segments processed into colored line segment chains}
    \label{fig_chains_before_after}
\end{figure}\\
Generating the polylines now would yield better results, but still create unusable data, because the line segments within each chain and the two points within each line segment are not sorted. The first point in a list of line segments in a chain could for example be a point in the middle of the chain, resulting in OpenCV's Polyline function to connect the following points in the wrong order.\\
The sorting algorithm for solving this problem has two parts: the first sorts the line segments from beginning of the chain to the end, the second sorts the end- and startpoint of each line segment individually, so that they too appear in order of 'flow' in the chain.

To differentiate between intermediate points and endpoints the function utilizes the way in which the points where found in the first place: when two line segments intersect to form a 90° degree turn, each segment consists of a start- and an endpoint. This means, there are always two points in close proximity at intermediate points, where line segments meet, but only one point at the endpoints, because there is only one line segment ending at each endpoint \ref{fig_point_zoom}.
Using this discrepancy, the function identifies endpoints by iterating through all points and checking for any other points within a five-pixel radius. Any point without any other points within this radius is considered an endpoint of a chain.\\
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Pictures/zoomed_in_points.png}
    \caption{Difference between intermediate points and endpoints}
    \label{fig_point_zoom}
\end{figure}
Using the identified endpoints to initialize the sorted\_chain list allows the program to organize the chains systematically. The process begins by selecting the segment that includes one of the start points as the initial segment of the chain. The endpoint of this segment that is not a chain endpoint is designated as the first last\_point in the sorted\_chain list. To determine the next segment in the chain, a lambda function calculates the distance between the last point and both points of each remaining segment. The segment containing the point with the smallest distance to the last\_point is selected as next\_segment and removed from the list of remaining\_segments. Within this segment, the point closest to the last\_point is appended first to the sorted\_chain, followed by the other point of the segment. This process repeats until no segments remain in the remaining\_segments list. The entire process is repeated for each chain until all points in all chains are ordered according to the edge's flow.

To generate polylines, ordered lists of points, from the sorted chains, the method iterates through the sorted\_chains list, appending each point in sequence. Once the polyline is constructed, it is converted into the format required by OpenCV for further processing.
\subsection{Further Improvements}
This method successfully detects and processes nearly all edges within XGEE. In contrast to the previous algorithm, it can identify edges in any orientation, regardless of their start or endpoint or order of their line segments. Furthermore, it is capable of processing and interpreting intersections and signal containers, making it well-suited for handling large, complex models. The method performs reliably across all three editor modes, provided the models are formatted correctly.
A notable problem arises when vertices with surrounding black edges are scaled sufficiently, making their edges resemble signal-carrying edges, which complicates their differentiation. In this case, a possible solution would be to let the vertex detection run first and exclude the found areas when applying the edge detection. Alternatively, the presence of colored pixels around the edges could be checked, as the background around edges is typically white, unlike the areas inside device or function vertices. This way, any obstructions caused by the vertices can be avoided.\\
Another issue may arise if the edges are configured by the user in an unexpected way. For instance, intersection within a few pixels of each other or intersections hidden behind vertices cannot be properly interpreted by the current method, which could lead to unexpected results. Future improvements of the XGEE editor, such as a more advanced automatic arrangement algorithm, could reduce or eliminate the risk of ambiguous user input. Additionally, enhancing the readability of subtask edges within the allocations editor would be beneficial for improving the verification tool. Currently, these edges often overlap with text, are thin and have poor contrast, making them difficult for the current edge detection pipeline to detect accurately.

\section{Vertex Detection}
Vertices represent distinct visual elements within XGEE such as functions, devices, containers or IO ports. Identifying these vertices is critical for interpreting the structural arrangement of diagrams. This section introduces a template-matching approach to address problems including overlapping vertices and varying sizes, ensuring a more reliable vertex detection in all relevant diagram types within XGEE.

A major problem in template matching is the diversity of vertices within XGEE's three editor models. For example, the functions editor contains only functions, while the hardware editor contains devices and IO ports and the allocations editor contains devices, signal containers, signal arrows and subtasks which overlap with devices and contain their own set of subvertices. Additionally, some vertices are scalable, while others are not. Scalable vertices require extra processing to ensure accurate detection. However, applying this processing universally to all vertices would result in unpredictable behavior.

\subsection{Methology}
To address these challenges, the list of unique vertices within the current editor model is analyzed to identify any vertices with the \textit{isSubVertexBody} flag set, which indicates they are subvertices, such as subtasks, that overlap with other vertices. These subvertices are prioritized and placed at the beginning of the list of vertices. This way, the vertex detection method can progressively simplify the image by erasing detected subvertices after the first iteration of the vertex detection function, allowing the underlying vertices to be detected without obstruction. Other relevant flags are: 
\begin{itemize}
    \item \textit{isScalable} which indicates whether the vertex can be scaled
    \item \textit{filepath} which specifies the path to the template image
    \item \textit{sizeX} and \textit{sizeY} which specify the size of the vertex
    \item \textit{parent\_filepath} which specifies the path to the parent vertex in case it it a subvertex
\end{itemize}
The method iterates through the list of unique vertices in the current editor model. If subvertices were identified in the previous step, they are processed first. They are removed from the image by setting all pixels within the bounding box of the subvertex to the \textit{fill} color of the parent vertex \ref{fig_draw_boxes}.
\begin{figure}[htb]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Pictures/draw_boxes_before.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Pictures/draw_boxes_after.png}
    \end{minipage}
    \caption{Original image from allocations editor containing subvertices (left) and modified image with subvertices removed (right).}
    \label{fig_draw_boxes}
\end{figure}
After processing the subvertices, a new image wrapper object is created using the modified image. The method then iterates through the list of remaining vertices, loading the template image of each vertex using the \textit{filepath} attribute. Depending on the \textit{isScalable} flag, an object of either the \textit{ScalableVertexDetector} or \textit{PortDetector} class is created and the corresponding vertex detection is applied.\\
In case of unscalable vertices, OpenCV's \textit{matchTemplate} and \textit{normalize} functions are applied. The resulting data is threshholded to identify the positions of all found vertices. To define the vertex boundaries, the \textit{sizeX} and \textit{sizeY} attributes are used, allowing bounding boxes of the correct size to be drawn with the detected match serving as the upper-left corner. This approach is effective for detecting vertices like IO ports and subtasks.\\
In case of scalable vertices, this approach does not work because the size of the vertex is not fixed, which reduces the certainty of found matches drastically and eliminates constant \textit{sizeX} and \textit{sizeY} values as a reliable method for defining bounding boxes.\\
Scalable Vertices within XGEE are Functions and Devices.
The initial template matching algorithm frequently exhibited inaccuracies when applied to large function containers. Specifically, it tended to produce higher similarity scores at the corners of the container, as the black border surrounding the vertex at these locations more closely resembled the black border in the template image. In contrast, the absence of a black border in the center of the container led to lower similarity scores. In extreme cases, this resulted in the center of the vertex not being detected at all, with the algorithm instead falsely identifying four smaller vertices at the container's corners. The same issue occurres with large devices.\\
To detect these vertices regardless of their dimensions, multiple templates of the same size for different parts of the large vertex are required.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{Pictures/templates.png}
    \caption{Templates for scalable vertex detection}
    \label{fig_templates}
\end{figure}
Template matching is performed iteratively with each template, and the results are compared after each iteration. The highest similarity score for each pixel is retained and combined into the resulting data, which is then threshholded to extract all potential matches.\\
Typically, numerous matches are identified during this process. To eliminate false positives and process the matches to determine the differently sized bounding boxes of the vertices, an algorithm described in \cite{og_paper} is applied.\\
The original image is thresholded to distinguish foreground from background pixels, as illustrated in \ref{fig_foreground_threshhold}. This foreground information is then utilized to filter the detected matches, retaining only those located within the foreground area. This process effectively removes false positives which often occur because the black pixels along the edges and borders of function- or device containers closely resemble those in the black borders in the template images.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\linewidth]{Pictures/foreground_threshhold.png}
    \caption{Image from allocations editor (left) with foreground threshhold applied (right).}
    \label{fig_foreground_threshhold}
\end{figure}
For each remaining detected match, the \textit{sizeX} and \textit{sizeY} attributes are used to create a filled bounding box around the detected position. Since multiple matches are often found at various locations within a single large vertex, the overlapping bounding boxes collectively form a larger structure. This structure is then simplified into a single bounding box using OpenCV's \textit{findContours} function.
\begin{figure}[htb]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Pictures/many_templates_before.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Pictures/many_templates_after.png}
    \end{minipage}
    \caption{Image from allocations editor containing a large, partially detected device using one template(left) and containing a large, fully detected device using multiple templates(right).}
    \label{fig_many_templates}
\end{figure}
This approach successfully detects all scalable vertices within XGEE, regardless of their dimensions. Using both scalable and non-scalable vertex detection methods ensures that all vertices are detected accurately, regardless of their size or complexity. The method is capable of detecting all vertices in the 10 test cases within this paper, including subvertices and scalable vertices.

\subsection{Further Improvements}
The current method is capable of reliably detecting all vertices within XGEE, including subvertices and scalable vertices. However, enhancing the reliability of subsequent steps in the visualization verification process could be achieved by erasing all detected vertices after processing. To implement this, the method would need to:
\begin{itemize}
    \item Process vertices in hierarchical order to prevent subvertices to be erased with their parent vertices before being detected by the method.
    \item Dynamically update the input image after each iteration.
    \item Correctly handle cases where subvertices, such as IO ports, only partially overlap with their parent vertices
\end{itemize}
This approach could also be adapted to erase other components, such as text and edges after their detection, as these elements can interfere with the accurate identification of remaining visualization components.\\
Furthermore, the method could be expanded to detect subvertices of subtasks, which are currently excluded from detection due to their small size. These \textit{subsubvertices} are challenging to differentiate from noise, character fragments, or edge segments using OpenCV. Employing image preprocessing techniques or refining or exchanging the template matching method could improve the detection of these smaller elements.

\section{Text Recognition}
The original text detection in XGEE utilized Pytesseract, an open-source optical character recognition (OCR) engine developed and sponsored by Google in 2006. Pytesseract required separate installation from other packages and could only detect text in images that had been preprocessed. Additionally, the output data required extensive postprocessing to become usable. While Pytesseract demonstrated high accuracy and speed, it struggled to reliably detect small text or text with low contrast to the background. Its optimization for structured text formats, such as those found in books, further hindered its performance in XGEE, where text can appear in varying orientations, sizes, and positions. These limitations made reliable text detection using Pytesseract difficult to achieve.
\subsection{Methology}
After evaluating various OCR engines, including EasyOCR \cite{easyocr_doc}, Doctr \cite{doctr_doc}, and Keras-OCR \cite{keras_ocr_doc}, EasyOCR proved to be the most suitable alternative.
EasyOCR is a deep-learning-based OCR engine that reliably detects text in images, even under challenging conditions such as low contrast or resolution. Although it operates more slowly on modern CPUs compared to some alternatives, it performs significantly faster on GPUs. Additionally, its ability to detect text in multiple languages adds potential value for future applications. Integration of EasyOCR into the XGEE editor for the user is straightforward, as it can be installed directly via pip install easyocr through the requirements.txt file without requiring additional dependencies or downloads. Unlike Pytesseract, EasyOCR only necessitates minimal image preprocessing, and it structures found characters into words and sentences automatically based on proximity, eliminating the need for extensive postprocessing. These advantages make EasyOCR the optimal choice for the updated text detection pipeline within XGEE.

First, a reader object is created and the language of the text is specified. The readtext function of the reader object is then called with the image as an argument. The image has to be padded to have a square shape, so there are no problems rotating the data later. To ensure a more reliable detection, the image is slightly blurred and upscaled to counter any aliasing and small characters. The text detection function then returns a list containing the detected text, its bounding box, and a certainty factor. Parameters can be specified when calling this function to adjust the expected text properties, enhancing the reliability of the detection. For example in earlier versions, EasyOCR struggled to detect single characters, but with these parameter adjustments, this issue has been resolved. The detection process is repeated for the rotated version of the image to ensure comprehensive results. This step is repeated with the rotated image. The positions of the bounding boxes of the found rotated text are then rotated around the center of the image to reallign them with the found text of the original image. Reading an image which contains rotated text usually results in many falsely read characters, because for example 'o' and 'l' can be interpreted regardless of orientation. To filter out the falsely red characters, the algorithm first combines the found results of both ocr-searches and then removes every word shorter than three letters.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Pictures/text_before.png}
    \caption{Input image from function editor containing rotated text}
    \label{fig_text_before}

    \centering
    \includegraphics[width=0.8\linewidth]{Pictures/text_after.png}
    \caption{Output image from function editor containing text bounding boxes}
    \label{fig_text_after}
\end{figure}
Because easyocr automatically groups detected characters into words and sentences, this is a simple and effective method to filter out falsely read characters which is able to detect all text in the 10 testcases within this paper. Using this approach also allows the same algorithm to be used for all images, regardless of the XGEE editor they were created in.
\subsection{Future Improvements}
In the future, this method could be enhanced by incorporating a more advanced algorithm to filter out incorrectly detected characters. Additionally, identifying the specific editor type could allow the system to selectively search for rotated characters only when they are expected to be present in the image.\\
Currently, the text recognition is the most time consuming component of the visualization verification process. Optimizing its performance would significantly reduce the overall processing time. This could be achieved by running the visualization verification on a gpu, parallelize the text detection to run simultaneous to the edge- and vertex detection or by using a more traditional text detection algorithm which does not require as much computational power as EasyOCR. These improvements would enable the system to process larger models more quickly and efficiently, addressing the current bottleneck in the pipeline.\\
